{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu using PyTorch 1.13.1+cpu and Flower 1.1.0\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import flwr as fl\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import CIFAR10\n",
    "from flwr.server.strategy import FedAvg\n",
    "from flwr.server.client_proxy import ClientProxy\n",
    "from flwr.common import FitRes, parameters_to_ndarrays, ndarrays_to_parameters\n",
    "from torchvision.models._api import Weights\n",
    "\n",
    "from tinysmpc import VirtualMachine, PrivateScalar\n",
    "from tinysmpc.fixed_point import fixed_point, float_point\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")  # Try \"cuda\" to train on GPU\n",
    "print(f\"Training on {DEVICE} using PyTorch {torch.__version__} and Flower {fl.__version__}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "NUM_CLIENTS = 10\n",
    "\n",
    "\n",
    "def load_datasets(num_clients: int):\n",
    "    # Download and transform CIFAR-10 (train and test)\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    )\n",
    "    trainset = CIFAR10(\"./dataset\", train=True, download=True, transform=transform)\n",
    "    testset = CIFAR10(\"./dataset\", train=False, download=True, transform=transform)\n",
    "\n",
    "    # Split training set into `num_clients` partitions to simulate different local datasets\n",
    "    partition_size = len(trainset) // num_clients\n",
    "    lengths = [partition_size] * num_clients\n",
    "    datasets = random_split(trainset, lengths, torch.Generator().manual_seed(42))\n",
    "\n",
    "    # Split each partition into train/val and create DataLoader\n",
    "    trainloaders = []\n",
    "    valloaders = []\n",
    "    for ds in datasets:\n",
    "        len_val = len(ds) // 10  # 10 % validation set\n",
    "        len_train = len(ds) - len_val\n",
    "        lengths = [len_train, len_val]\n",
    "        ds_train, ds_val = random_split(ds, lengths, torch.Generator().manual_seed(42))\n",
    "        trainloaders.append(DataLoader(ds_train, batch_size=32, shuffle=True))\n",
    "        valloaders.append(DataLoader(ds_val, batch_size=32))\n",
    "    testloader = DataLoader(testset, batch_size=32)\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "\n",
    "trainloaders, valloaders, testloader = load_datasets(NUM_CLIENTS)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_parameters(net) -> List[np.ndarray]:\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "\n",
    "\n",
    "def set_parameters(net, parameters: List[np.ndarray]):\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "\n",
    "def train(net, trainloader, epochs: int):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    net.train()\n",
    "    for epoch in range(epochs):\n",
    "        correct, total, epoch_loss = 0, 0, 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "            loss = criterion(net(images), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Metrics\n",
    "            epoch_loss += loss\n",
    "            total += labels.size(0)\n",
    "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "        epoch_loss /= len(trainloader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "        print(f\"Epoch {epoch + 1}: train loss {epoch_loss}, accuracy {epoch_acc}\")\n",
    "\n",
    "\n",
    "def test(net, testloader):\n",
    "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    loss /= len(testloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return loss, accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, cid, net, trainloader, valloader):\n",
    "        self.cid = cid\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        print(f\"[Client {self.cid}] get_parameters\")\n",
    "        return get_parameters(self.net)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        print(f\"[Client {self.cid}] fit, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        train(self.net, self.trainloader, epochs=1)\n",
    "        return get_parameters(self.net), len(self.trainloader), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        print(f\"[Client {self.cid}] evaluate, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        loss, accuracy = test(self.net, self.valloader)\n",
    "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}\n",
    "\n",
    "\n",
    "def client_fn(cid) -> FlowerClient:\n",
    "    net = Net().to(DEVICE)\n",
    "    trainloader = trainloaders[int(cid)]\n",
    "    valloader = valloaders[int(cid)]\n",
    "    return FlowerClient(cid, net, trainloader, valloader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flower 2023-01-24 23:12:16,665 | app.py:140 | Starting Flower simulation, config: ServerConfig(num_rounds=2, round_timeout=None)\n",
      "2023-01-24 23:12:18,067\tINFO worker.py:1518 -- Started a local Ray instance.\n",
      "INFO flower 2023-01-24 23:12:21,008 | app.py:174 | Flower VCE: Ray initialized with resources: {'node:127.0.0.1': 1.0, 'CPU': 4.0, 'object_store_memory': 2303655936.0, 'memory': 4607311872.0}\n",
      "INFO flower 2023-01-24 23:12:21,009 | server.py:86 | Initializing global parameters\n",
      "INFO flower 2023-01-24 23:12:21,011 | server.py:270 | Requesting initial parameters from one random client\n",
      "INFO flower 2023-01-24 23:12:24,013 | server.py:274 | Received initial parameters from one random client\n",
      "INFO flower 2023-01-24 23:12:24,014 | server.py:88 | Evaluating initial parameters\n",
      "INFO flower 2023-01-24 23:12:24,014 | server.py:101 | FL starting\n",
      "DEBUG flower 2023-01-24 23:12:24,015 | server.py:215 | fit_round 1: strategy sampled 2 clients (out of 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(launch_and_get_parameters pid=13384)\u001B[0m [Client 0] get_parameters\n",
      "\u001B[2m\u001B[36m(launch_and_fit pid=13384)\u001B[0m [Client 0] fit, config: {}\n",
      "\u001B[2m\u001B[36m(launch_and_fit pid=3728)\u001B[0m [Client 1] fit, config: {}\n",
      "\u001B[2m\u001B[36m(launch_and_fit pid=13384)\u001B[0m Epoch 1: train loss 0.06490003317594528, accuracy 0.2331111111111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flower 2023-01-24 23:12:30,734 | server.py:229 | fit_round 1 received 2 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(launch_and_fit pid=3728)\u001B[0m Epoch 1: train loss 0.06493470817804337, accuracy 0.23044444444444445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING flower 2023-01-24 23:12:31,066 | fedavg.py:242 | No fit_metrics_aggregation_fn provided\n",
      "DEBUG flower 2023-01-24 23:12:31,067 | server.py:165 | evaluate_round 1: strategy sampled 2 clients (out of 2)\n",
      "DEBUG flower 2023-01-24 23:12:33,375 | server.py:179 | evaluate_round 1 received 2 results and 0 failures\n",
      "WARNING flower 2023-01-24 23:12:33,375 | fedavg.py:273 | No evaluate_metrics_aggregation_fn provided\n",
      "DEBUG flower 2023-01-24 23:12:33,376 | server.py:215 | fit_round 2: strategy sampled 2 clients (out of 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(launch_and_evaluate pid=13384)\u001B[0m [Client 0] evaluate, config: {}\n",
      "\u001B[2m\u001B[36m(launch_and_evaluate pid=3728)\u001B[0m [Client 1] evaluate, config: {}\n",
      "\u001B[2m\u001B[36m(launch_and_fit pid=13384)\u001B[0m [Client 1] fit, config: {}\n",
      "\u001B[2m\u001B[36m(launch_and_fit pid=3728)\u001B[0m [Client 0] fit, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flower 2023-01-24 23:12:38,179 | server.py:229 | fit_round 2 received 2 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "\u001B[2m\u001B[36m(launch_and_fit pid=13384)\u001B[0m Epoch 1: train loss nan, accuracy 0.09733333333333333\n",
      "\u001B[2m\u001B[36m(launch_and_fit pid=3728)\u001B[0m Epoch 1: train loss nan, accuracy 0.10533333333333333\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 68\u001B[0m\n\u001B[0;32m     63\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39maggregate_fit(rnd, results, failures)\n\u001B[0;32m     66\u001B[0m strategy \u001B[38;5;241m=\u001B[39m FedAvgSmc(fraction_fit\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.3\u001B[39m, fraction_evaluate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.3\u001B[39m, min_fit_clients\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, min_evaluate_clients\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m,\n\u001B[0;32m     67\u001B[0m                      min_available_clients\u001B[38;5;241m=\u001B[39mNUM_CLIENTS)\n\u001B[1;32m---> 68\u001B[0m \u001B[43mfl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msimulation\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart_simulation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclient_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclient_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_clients\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mserver\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mServerConfig\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_rounds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     69\u001B[0m \u001B[43m                               \u001B[49m\u001B[43mstrategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mFedAvgSmc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\machine_learning_0\\lib\\site-packages\\flwr\\simulation\\app.py:191\u001B[0m, in \u001B[0;36mstart_simulation\u001B[1;34m(client_fn, num_clients, clients_ids, client_resources, server, config, strategy, client_manager, ray_init_args, keep_initialised)\u001B[0m\n\u001B[0;32m    188\u001B[0m     initialized_server\u001B[38;5;241m.\u001B[39mclient_manager()\u001B[38;5;241m.\u001B[39mregister(client\u001B[38;5;241m=\u001B[39mclient_proxy)\n\u001B[0;32m    190\u001B[0m \u001B[38;5;66;03m# Start training\u001B[39;00m\n\u001B[1;32m--> 191\u001B[0m hist \u001B[38;5;241m=\u001B[39m \u001B[43m_fl\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    192\u001B[0m \u001B[43m    \u001B[49m\u001B[43mserver\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minitialized_server\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    193\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minitialized_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    194\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    196\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m hist\n",
      "File \u001B[1;32m~\\.conda\\envs\\machine_learning_0\\lib\\site-packages\\flwr\\server\\app.py:191\u001B[0m, in \u001B[0;36m_fl\u001B[1;34m(server, config)\u001B[0m\n\u001B[0;32m    186\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_fl\u001B[39m(\n\u001B[0;32m    187\u001B[0m     server: Server,\n\u001B[0;32m    188\u001B[0m     config: ServerConfig,\n\u001B[0;32m    189\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m History:\n\u001B[0;32m    190\u001B[0m     \u001B[38;5;66;03m# Fit model\u001B[39;00m\n\u001B[1;32m--> 191\u001B[0m     hist \u001B[38;5;241m=\u001B[39m \u001B[43mserver\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_rounds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_rounds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mround_timeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    192\u001B[0m     log(INFO, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapp_fit: losses_distributed \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mstr\u001B[39m(hist\u001B[38;5;241m.\u001B[39mlosses_distributed))\n\u001B[0;32m    193\u001B[0m     log(INFO, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapp_fit: metrics_distributed \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mstr\u001B[39m(hist\u001B[38;5;241m.\u001B[39mmetrics_distributed))\n",
      "File \u001B[1;32m~\\.conda\\envs\\machine_learning_0\\lib\\site-packages\\flwr\\server\\server.py:106\u001B[0m, in \u001B[0;36mServer.fit\u001B[1;34m(self, num_rounds, timeout)\u001B[0m\n\u001B[0;32m    102\u001B[0m start_time \u001B[38;5;241m=\u001B[39m timeit\u001B[38;5;241m.\u001B[39mdefault_timer()\n\u001B[0;32m    104\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m current_round \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, num_rounds \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m    105\u001B[0m     \u001B[38;5;66;03m# Train model and replace previous global model\u001B[39;00m\n\u001B[1;32m--> 106\u001B[0m     res_fit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_round\u001B[49m\u001B[43m(\u001B[49m\u001B[43mserver_round\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurrent_round\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    107\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m res_fit:\n\u001B[0;32m    108\u001B[0m         parameters_prime, _, _ \u001B[38;5;241m=\u001B[39m res_fit  \u001B[38;5;66;03m# fit_metrics_aggregated\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\machine_learning_0\\lib\\site-packages\\flwr\\server\\server.py:241\u001B[0m, in \u001B[0;36mServer.fit_round\u001B[1;34m(self, server_round, timeout)\u001B[0m\n\u001B[0;32m    229\u001B[0m log(\n\u001B[0;32m    230\u001B[0m     DEBUG,\n\u001B[0;32m    231\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfit_round \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m received \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m results and \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m failures\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    234\u001B[0m     \u001B[38;5;28mlen\u001B[39m(failures),\n\u001B[0;32m    235\u001B[0m )\n\u001B[0;32m    237\u001B[0m \u001B[38;5;66;03m# Aggregate training results\u001B[39;00m\n\u001B[0;32m    238\u001B[0m aggregated_result: Tuple[\n\u001B[0;32m    239\u001B[0m     Optional[Parameters],\n\u001B[0;32m    240\u001B[0m     Dict[\u001B[38;5;28mstr\u001B[39m, Scalar],\n\u001B[1;32m--> 241\u001B[0m ] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstrategy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maggregate_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mserver_round\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresults\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfailures\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    243\u001B[0m parameters_aggregated, metrics_aggregated \u001B[38;5;241m=\u001B[39m aggregated_result\n\u001B[0;32m    244\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m parameters_aggregated, metrics_aggregated, (results, failures)\n",
      "Cell \u001B[1;32mIn[5], line 37\u001B[0m, in \u001B[0;36mFedAvgSmc.aggregate_fit\u001B[1;34m(self, rnd, results, failures)\u001B[0m\n\u001B[0;32m     35\u001B[0m     layers_weights[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlayer_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     36\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m weights \u001B[38;5;129;01min\u001B[39;00m fit_res_ndarray_parameters:\n\u001B[1;32m---> 37\u001B[0m         layers_weights[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlayer_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(\u001B[43mfixedPoint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweights\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlayer\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_layers):\n\u001B[0;32m     40\u001B[0m     fl_node_values \u001B[38;5;241m=\u001B[39m [PrivateScalar(tensor, node) \u001B[38;5;28;01mfor\u001B[39;00m tensor, node \u001B[38;5;129;01min\u001B[39;00m\n\u001B[0;32m     41\u001B[0m                       \u001B[38;5;28mzip\u001B[39m(layers_weights[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlayer_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m], fl_nodes)]\n",
      "File \u001B[1;32m~\\.conda\\envs\\machine_learning_0\\lib\\site-packages\\numpy\\lib\\function_base.py:2328\u001B[0m, in \u001B[0;36mvectorize.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2325\u001B[0m     vargs \u001B[38;5;241m=\u001B[39m [args[_i] \u001B[38;5;28;01mfor\u001B[39;00m _i \u001B[38;5;129;01min\u001B[39;00m inds]\n\u001B[0;32m   2326\u001B[0m     vargs\u001B[38;5;241m.\u001B[39mextend([kwargs[_n] \u001B[38;5;28;01mfor\u001B[39;00m _n \u001B[38;5;129;01min\u001B[39;00m names])\n\u001B[1;32m-> 2328\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_vectorize_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\machine_learning_0\\lib\\site-packages\\numpy\\lib\\function_base.py:2406\u001B[0m, in \u001B[0;36mvectorize._vectorize_call\u001B[1;34m(self, func, args)\u001B[0m\n\u001B[0;32m   2404\u001B[0m     res \u001B[38;5;241m=\u001B[39m func()\n\u001B[0;32m   2405\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2406\u001B[0m     ufunc, otypes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_ufunc_and_otypes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2408\u001B[0m     \u001B[38;5;66;03m# Convert args to object arrays first\u001B[39;00m\n\u001B[0;32m   2409\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m [asanyarray(a, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mobject\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m args]\n",
      "File \u001B[1;32m~\\.conda\\envs\\machine_learning_0\\lib\\site-packages\\numpy\\lib\\function_base.py:2366\u001B[0m, in \u001B[0;36mvectorize._get_ufunc_and_otypes\u001B[1;34m(self, func, args)\u001B[0m\n\u001B[0;32m   2362\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcannot call `vectorize` on size 0 inputs \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m   2363\u001B[0m                      \u001B[38;5;124m'\u001B[39m\u001B[38;5;124munless `otypes` is set\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m   2365\u001B[0m inputs \u001B[38;5;241m=\u001B[39m [arg\u001B[38;5;241m.\u001B[39mflat[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m arg \u001B[38;5;129;01min\u001B[39;00m args]\n\u001B[1;32m-> 2366\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2368\u001B[0m \u001B[38;5;66;03m# Performance note: profiling indicates that -- for simple\u001B[39;00m\n\u001B[0;32m   2369\u001B[0m \u001B[38;5;66;03m# functions at least -- this wrapping can almost double the\u001B[39;00m\n\u001B[0;32m   2370\u001B[0m \u001B[38;5;66;03m# execution time.\u001B[39;00m\n\u001B[0;32m   2371\u001B[0m \u001B[38;5;66;03m# Hence we make it optional.\u001B[39;00m\n\u001B[0;32m   2372\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache:\n",
      "File \u001B[1;32m~\\Documents\\uOttawa\\Thesis\\federated_learning\\test_bench\\tinysmpc\\fixed_point.py:16\u001B[0m, in \u001B[0;36mfixed_point\u001B[1;34m(fl)\u001B[0m\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;28mprint\u001B[39m(fl)\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# assert MIN_FLOAT < fl < MAX_FLOAT\u001B[39;00m\n\u001B[1;32m---> 16\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfl\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mPRECISION\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mValueError\u001B[0m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "Q = 2657003489534545107915232808830590043\n",
    "fixedPoint = np.vectorize(fixed_point)\n",
    "floatPoint = np.vectorize(float_point)\n",
    "\n",
    "\n",
    "class FedAvgSmc(FedAvg):\n",
    "    def aggregate_fit(\n",
    "            self,\n",
    "            rnd: int,\n",
    "            results: List[Tuple[fl.server.client_proxy.ClientProxy, FitRes]],\n",
    "            failures: List[BaseException],\n",
    "    ) -> Optional[Weights]:\n",
    "        if not results:\n",
    "            return None\n",
    "\n",
    "        clients = []\n",
    "        fit_results = []\n",
    "\n",
    "        for client, fit_result in results:\n",
    "            clients.append(client)\n",
    "            fit_results.append(fit_result)\n",
    "\n",
    "        fit_res_ndarray_parameters = [parameters_to_ndarrays(result.parameters) for result in\n",
    "                                      fit_results]  # list of clients, each with a list of layers(each layer representing weights)\n",
    "\n",
    "        fl_nodes = [VirtualMachine(f\"Client: {client.cid}\") for client in clients]\n",
    "\n",
    "        num_layers = len(fit_results[0].parameters.tensors)\n",
    "\n",
    "        layers_weights = {}\n",
    "\n",
    "        layers_weights_smc = {}\n",
    "\n",
    "        for layer in range(num_layers):  #loop through number of layers\n",
    "            layers_weights[f\"layer_{layer}\"] = []\n",
    "            for weights in fit_res_ndarray_parameters:\n",
    "                layers_weights[f\"layer_{layer}\"].append(fixedPoint(weights[layer]))\n",
    "\n",
    "        for layer in range(num_layers):\n",
    "            fl_node_values = [PrivateScalar(tensor, node) for tensor, node in\n",
    "                              zip(layers_weights[f'layer_{layer}'], fl_nodes)]\n",
    "\n",
    "            fl_exchanged_shares = []\n",
    "            fl_exchanged_shares_list = []\n",
    "            layers_weights_smc[f\"layer_{layer}\"] = []\n",
    "\n",
    "            for value in fl_node_values:\n",
    "                fl_exchanged_shares.append(value.share_tensor(fl_nodes, Q))\n",
    "\n",
    "            for client_shares in fl_exchanged_shares:\n",
    "                fl_exchanged_shares_list = [share.value for share in client_shares.shares]\n",
    "\n",
    "            layers_weights_smc[f'layer_{layer}'] = floatPoint(fl_exchanged_shares_list)\n",
    "\n",
    "        for i, client in enumerate(clients):\n",
    "            client_weights = []\n",
    "            for layer in layers_weights_smc.values():\n",
    "                client_weights.append(np.array(layer[i]))\n",
    "            fit_results[i].parameters = ndarrays_to_parameters(client_weights)\n",
    "\n",
    "        # results = tuple(zip(clients, fit_results))\n",
    "\n",
    "        return super().aggregate_fit(rnd, results, failures)\n",
    "\n",
    "\n",
    "strategy = FedAvgSmc(fraction_fit=0.3, fraction_evaluate=0.3, min_fit_clients=3, min_evaluate_clients=3,\n",
    "                     min_available_clients=NUM_CLIENTS)\n",
    "fl.simulation.start_simulation(client_fn=client_fn, num_clients=2, config=fl.server.ServerConfig(num_rounds=2),\n",
    "                               strategy=FedAvgSmc(), )"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
