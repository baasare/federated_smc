{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu using PyTorch 1.13.1+cpu and Flower 1.1.0\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import flwr as fl\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import CIFAR10\n",
    "from flwr.server.strategy import FedAvg\n",
    "from flwr.server.client_proxy import ClientProxy\n",
    "from flwr.common import FitRes, parameters_to_ndarrays\n",
    "from torchvision.models._api import Weights\n",
    "\n",
    "from tinysmpc import VirtualMachine, PrivateScalar\n",
    "from tinysmpc.fixed_point import fixed_point, float_point\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")  # Try \"cuda\" to train on GPU\n",
    "print(f\"Training on {DEVICE} using PyTorch {torch.__version__} and Flower {fl.__version__}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "NUM_CLIENTS = 10\n",
    "\n",
    "\n",
    "def load_datasets(num_clients: int):\n",
    "    # Download and transform CIFAR-10 (train and test)\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    )\n",
    "    trainset = CIFAR10(\"./dataset\", train=True, download=True, transform=transform)\n",
    "    testset = CIFAR10(\"./dataset\", train=False, download=True, transform=transform)\n",
    "\n",
    "    # Split training set into `num_clients` partitions to simulate different local datasets\n",
    "    partition_size = len(trainset) // num_clients\n",
    "    lengths = [partition_size] * num_clients\n",
    "    datasets = random_split(trainset, lengths, torch.Generator().manual_seed(42))\n",
    "\n",
    "    # Split each partition into train/val and create DataLoader\n",
    "    trainloaders = []\n",
    "    valloaders = []\n",
    "    for ds in datasets:\n",
    "        len_val = len(ds) // 10  # 10 % validation set\n",
    "        len_train = len(ds) - len_val\n",
    "        lengths = [len_train, len_val]\n",
    "        ds_train, ds_val = random_split(ds, lengths, torch.Generator().manual_seed(42))\n",
    "        trainloaders.append(DataLoader(ds_train, batch_size=32, shuffle=True))\n",
    "        valloaders.append(DataLoader(ds_val, batch_size=32))\n",
    "    testloader = DataLoader(testset, batch_size=32)\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "\n",
    "trainloaders, valloaders, testloader = load_datasets(NUM_CLIENTS)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_parameters(net) -> List[np.ndarray]:\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "\n",
    "\n",
    "def set_parameters(net, parameters: List[np.ndarray]):\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "\n",
    "def train(net, trainloader, epochs: int):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    net.train()\n",
    "    for epoch in range(epochs):\n",
    "        correct, total, epoch_loss = 0, 0, 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "            loss = criterion(net(images), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Metrics\n",
    "            epoch_loss += loss\n",
    "            total += labels.size(0)\n",
    "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "        epoch_loss /= len(trainloader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "        print(f\"Epoch {epoch + 1}: train loss {epoch_loss}, accuracy {epoch_acc}\")\n",
    "\n",
    "\n",
    "def test(net, testloader):\n",
    "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    loss /= len(testloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return loss, accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, cid, net, trainloader, valloader):\n",
    "        self.cid = cid\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        print(f\"[Client {self.cid}] get_parameters\")\n",
    "        return get_parameters(self.net)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        print(f\"[Client {self.cid}] fit, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        train(self.net, self.trainloader, epochs=1)\n",
    "        return get_parameters(self.net), len(self.trainloader), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        print(f\"[Client {self.cid}] evaluate, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        loss, accuracy = test(self.net, self.valloader)\n",
    "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}\n",
    "\n",
    "\n",
    "def client_fn(cid) -> FlowerClient:\n",
    "    net = Net().to(DEVICE)\n",
    "    trainloader = trainloaders[int(cid)]\n",
    "    valloader = valloaders[int(cid)]\n",
    "    return FlowerClient(cid, net, trainloader, valloader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flower 2023-01-24 20:57:15,309 | app.py:140 | Starting Flower simulation, config: ServerConfig(num_rounds=1, round_timeout=None)\n",
      "2023-01-24 20:57:19,352\tINFO worker.py:1518 -- Started a local Ray instance.\n",
      "INFO flower 2023-01-24 20:57:22,716 | app.py:174 | Flower VCE: Ray initialized with resources: {'memory': 4115253659.0, 'node:127.0.0.1': 1.0, 'object_store_memory': 2057626828.0, 'CPU': 4.0}\n",
      "INFO flower 2023-01-24 20:57:22,717 | server.py:86 | Initializing global parameters\n",
      "INFO flower 2023-01-24 20:57:22,719 | server.py:270 | Requesting initial parameters from one random client\n",
      "INFO flower 2023-01-24 20:57:26,044 | server.py:274 | Received initial parameters from one random client\n",
      "INFO flower 2023-01-24 20:57:26,045 | server.py:88 | Evaluating initial parameters\n",
      "INFO flower 2023-01-24 20:57:26,045 | server.py:101 | FL starting\n",
      "DEBUG flower 2023-01-24 20:57:26,046 | server.py:215 | fit_round 1: strategy sampled 2 clients (out of 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(launch_and_get_parameters pid=8632)\u001B[0m [Client 0] get_parameters\n",
      "\u001B[2m\u001B[36m(launch_and_fit pid=8632)\u001B[0m [Client 1] fit, config: {}\n",
      "\u001B[2m\u001B[36m(launch_and_fit pid=13692)\u001B[0m [Client 0] fit, config: {}\n",
      "\u001B[2m\u001B[36m(launch_and_fit pid=8632)\u001B[0m Epoch 1: train loss 0.06433320045471191, accuracy 0.22933333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flower 2023-01-24 20:57:33,821 | server.py:229 | fit_round 1 received 2 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SECURE MULTIPARTY COMPUTATION\n",
      "\u001B[2m\u001B[36m(launch_and_fit pid=13692)\u001B[0m Epoch 1: train loss 0.06410910934209824, accuracy 0.2311111111111111\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 450 into shape (1,6,3)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[55], line 65\u001B[0m\n\u001B[0;32m     60\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39maggregate_fit(rnd, results, failures)\n\u001B[0;32m     63\u001B[0m strategy \u001B[38;5;241m=\u001B[39m FedAvgSmc(fraction_fit\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.3\u001B[39m, fraction_evaluate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.3\u001B[39m, min_fit_clients\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, min_evaluate_clients\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m,\n\u001B[0;32m     64\u001B[0m                      min_available_clients\u001B[38;5;241m=\u001B[39mNUM_CLIENTS)\n\u001B[1;32m---> 65\u001B[0m \u001B[43mfl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msimulation\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart_simulation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclient_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclient_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_clients\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mserver\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mServerConfig\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_rounds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     66\u001B[0m \u001B[43m                               \u001B[49m\u001B[43mstrategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mFedAvgSmc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\machine_learning_0\\lib\\site-packages\\flwr\\simulation\\app.py:191\u001B[0m, in \u001B[0;36mstart_simulation\u001B[1;34m(client_fn, num_clients, clients_ids, client_resources, server, config, strategy, client_manager, ray_init_args, keep_initialised)\u001B[0m\n\u001B[0;32m    188\u001B[0m     initialized_server\u001B[38;5;241m.\u001B[39mclient_manager()\u001B[38;5;241m.\u001B[39mregister(client\u001B[38;5;241m=\u001B[39mclient_proxy)\n\u001B[0;32m    190\u001B[0m \u001B[38;5;66;03m# Start training\u001B[39;00m\n\u001B[1;32m--> 191\u001B[0m hist \u001B[38;5;241m=\u001B[39m \u001B[43m_fl\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    192\u001B[0m \u001B[43m    \u001B[49m\u001B[43mserver\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minitialized_server\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    193\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minitialized_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    194\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    196\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m hist\n",
      "File \u001B[1;32m~\\.conda\\envs\\machine_learning_0\\lib\\site-packages\\flwr\\server\\app.py:191\u001B[0m, in \u001B[0;36m_fl\u001B[1;34m(server, config)\u001B[0m\n\u001B[0;32m    186\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_fl\u001B[39m(\n\u001B[0;32m    187\u001B[0m     server: Server,\n\u001B[0;32m    188\u001B[0m     config: ServerConfig,\n\u001B[0;32m    189\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m History:\n\u001B[0;32m    190\u001B[0m     \u001B[38;5;66;03m# Fit model\u001B[39;00m\n\u001B[1;32m--> 191\u001B[0m     hist \u001B[38;5;241m=\u001B[39m \u001B[43mserver\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_rounds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_rounds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mround_timeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    192\u001B[0m     log(INFO, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapp_fit: losses_distributed \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mstr\u001B[39m(hist\u001B[38;5;241m.\u001B[39mlosses_distributed))\n\u001B[0;32m    193\u001B[0m     log(INFO, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapp_fit: metrics_distributed \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mstr\u001B[39m(hist\u001B[38;5;241m.\u001B[39mmetrics_distributed))\n",
      "File \u001B[1;32m~\\.conda\\envs\\machine_learning_0\\lib\\site-packages\\flwr\\server\\server.py:106\u001B[0m, in \u001B[0;36mServer.fit\u001B[1;34m(self, num_rounds, timeout)\u001B[0m\n\u001B[0;32m    102\u001B[0m start_time \u001B[38;5;241m=\u001B[39m timeit\u001B[38;5;241m.\u001B[39mdefault_timer()\n\u001B[0;32m    104\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m current_round \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, num_rounds \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m    105\u001B[0m     \u001B[38;5;66;03m# Train model and replace previous global model\u001B[39;00m\n\u001B[1;32m--> 106\u001B[0m     res_fit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_round\u001B[49m\u001B[43m(\u001B[49m\u001B[43mserver_round\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurrent_round\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    107\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m res_fit:\n\u001B[0;32m    108\u001B[0m         parameters_prime, _, _ \u001B[38;5;241m=\u001B[39m res_fit  \u001B[38;5;66;03m# fit_metrics_aggregated\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\machine_learning_0\\lib\\site-packages\\flwr\\server\\server.py:241\u001B[0m, in \u001B[0;36mServer.fit_round\u001B[1;34m(self, server_round, timeout)\u001B[0m\n\u001B[0;32m    229\u001B[0m log(\n\u001B[0;32m    230\u001B[0m     DEBUG,\n\u001B[0;32m    231\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfit_round \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m received \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m results and \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m failures\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    234\u001B[0m     \u001B[38;5;28mlen\u001B[39m(failures),\n\u001B[0;32m    235\u001B[0m )\n\u001B[0;32m    237\u001B[0m \u001B[38;5;66;03m# Aggregate training results\u001B[39;00m\n\u001B[0;32m    238\u001B[0m aggregated_result: Tuple[\n\u001B[0;32m    239\u001B[0m     Optional[Parameters],\n\u001B[0;32m    240\u001B[0m     Dict[\u001B[38;5;28mstr\u001B[39m, Scalar],\n\u001B[1;32m--> 241\u001B[0m ] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstrategy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maggregate_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mserver_round\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresults\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfailures\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    243\u001B[0m parameters_aggregated, metrics_aggregated \u001B[38;5;241m=\u001B[39m aggregated_result\n\u001B[0;32m    244\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m parameters_aggregated, metrics_aggregated, (results, failures)\n",
      "Cell \u001B[1;32mIn[55], line 48\u001B[0m, in \u001B[0;36mFedAvgSmc.aggregate_fit\u001B[1;34m(self, rnd, results, failures)\u001B[0m\n\u001B[0;32m     45\u001B[0m fl_exchanged_shares \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m value \u001B[38;5;129;01min\u001B[39;00m fl_node_values:\n\u001B[1;32m---> 48\u001B[0m     fl_exchanged_shares\u001B[38;5;241m.\u001B[39mappend(\u001B[43mvalue\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshare_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfl_nodes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mQ\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     50\u001B[0m fl_exchanged_shares_list \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m client, client_shares \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(fl_exchanged_shares):\n",
      "File \u001B[1;32m~\\Documents\\uOttawa\\Thesis\\federated_learning\\test_bench\\tinysmpc\\tinysmpc.py:41\u001B[0m, in \u001B[0;36mPrivateScalar.share_tensor\u001B[1;34m(self, machines, Q)\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mshare_tensor\u001B[39m(\u001B[38;5;28mself\u001B[39m, machines, Q\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m     40\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m'''Split self.value into secret shares and distribute them across machines (tracked in a SharedScalar).'''\u001B[39;00m\n\u001B[1;32m---> 41\u001B[0m     shares \u001B[38;5;241m=\u001B[39m \u001B[43mn_to_tensor_shares\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmachines\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mQ\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     42\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m SharedScalar(shares, Q)\n",
      "File \u001B[1;32m~\\Documents\\uOttawa\\Thesis\\federated_learning\\test_bench\\tinysmpc\\secret_sharing.py:107\u001B[0m, in \u001B[0;36mn_to_tensor_shares\u001B[1;34m(tensor, owners, Q)\u001B[0m\n\u001B[0;32m    105\u001B[0m \u001B[38;5;66;03m# Generate the value of each secret share using additive secret sharing\u001B[39;00m\n\u001B[0;32m    106\u001B[0m random_values \u001B[38;5;241m=\u001B[39m [random\u001B[38;5;241m.\u001B[39mrandrange(Q) \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(np\u001B[38;5;241m.\u001B[39mprod((tensor\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;241m+\u001B[39m (\u001B[38;5;28mlen\u001B[39m(owners) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m,))))]\n\u001B[1;32m--> 107\u001B[0m random_shares \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrandom_values\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mowners\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtensor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtensor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    109\u001B[0m values \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mconcatenate([random_shares, [(tensor \u001B[38;5;241m-\u001B[39m random_shares\u001B[38;5;241m.\u001B[39msum(axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)) \u001B[38;5;241m%\u001B[39m Q]])\n\u001B[0;32m    111\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tensor_dim_1:\n",
      "\u001B[1;31mValueError\u001B[0m: cannot reshape array of size 450 into shape (1,6,3)"
     ]
    }
   ],
   "source": [
    "Q = 2657003489534545107915232808830590043\n",
    "fixedPoint = np.vectorize(fixed_point)\n",
    "floatPoint = np.vectorize(float_point)\n",
    "\n",
    "\n",
    "class FedAvgSmc(FedAvg):\n",
    "    def aggregate_fit(\n",
    "            self,\n",
    "            rnd: int,\n",
    "            results: List[Tuple[fl.server.client_proxy.ClientProxy, FitRes]],\n",
    "            failures: List[BaseException],\n",
    "    ) -> Optional[Weights]:\n",
    "        if not results:\n",
    "            return None\n",
    "\n",
    "        print(\"SECURE MULTIPARTY COMPUTATION\")\n",
    "\n",
    "        clients = []\n",
    "        fit_results = []\n",
    "\n",
    "        for client, fit_result in results:\n",
    "            clients.append(client)\n",
    "            fit_results.append(fit_result)\n",
    "\n",
    "        fit_res_ndarray_parameters = [parameters_to_ndarrays(result.parameters) for result in fit_results]\n",
    "\n",
    "\n",
    "        num_layers = len(fit_results[0].parameters.tensors)\n",
    "        layers_weights = {}\n",
    "\n",
    "        layers_weights_smc = {}\n",
    "\n",
    "        for layer in range(num_layers): #loop through number of layers\n",
    "            layers_weights[f\"layer_{layer}\"] = []\n",
    "            for client, weights in enumerate(fit_res_ndarray_parameters):\n",
    "                layers_weights[f\"layer_{layer}\"].append(fixedPoint(weights[layer]))\n",
    "\n",
    "        # Virtual FL Edge Devices\n",
    "        fl_nodes = [VirtualMachine(f\"Client: {client.cid}\") for client, _ in results]\n",
    "\n",
    "        for layer in range(num_layers):\n",
    "            fl_node_values = [PrivateScalar(tensor, node) for tensor, node in zip(layers_weights[f'layer_{layer}'], fl_nodes)]\n",
    "\n",
    "            # Nodes with secret shares\n",
    "            fl_exchanged_shares = []\n",
    "\n",
    "            for value in fl_node_values:\n",
    "                fl_exchanged_shares.append(value.share_tensor(fl_nodes, Q))\n",
    "\n",
    "            fl_exchanged_shares_list = []\n",
    "\n",
    "            for client, client_shares in enumerate(fl_exchanged_shares):\n",
    "                temp = [share.value for share in client_shares.shares]\n",
    "                fl_exchanged_shares_list.append(floatPoint(temp))\n",
    "\n",
    "            layers_weights_smc[f'layer_{layer}'].append(fl_exchanged_shares_list)\n",
    "\n",
    "        print(f\"SMC LAYERS: {len(layers_weights_smc)}\")\n",
    "\n",
    "        return super().aggregate_fit(rnd, results, failures)\n",
    "\n",
    "\n",
    "strategy = FedAvgSmc(fraction_fit=0.3, fraction_evaluate=0.3, min_fit_clients=3, min_evaluate_clients=3,\n",
    "                     min_available_clients=NUM_CLIENTS)\n",
    "fl.simulation.start_simulation(client_fn=client_fn, num_clients=2, config=fl.server.ServerConfig(num_rounds=1),\n",
    "                               strategy=FedAvgSmc(), )"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
