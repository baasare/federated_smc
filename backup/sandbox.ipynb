{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu using PyTorch 1.13.1+cpu and Flower 1.1.0\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tinysmpc.tinysmpc import VirtualMachine, PrivateScalar\n",
    "from tinysmpc.tinysmpc.fixed_point import float_point, fixed_point\n",
    "\n",
    "import torch\n",
    "import flwr as fl\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import random_split, TensorDataset, DataLoader\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")  # Try \"cuda\" to train on GPU\n",
    "print(f\"Training on {DEVICE} using PyTorch {torch.__version__} and Flower {fl.__version__}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "N = 4\n",
    "Q = 2657003489534545107915232808830590043"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def additive_share(secret, N):\n",
    "    shares = [random.randrange(Q) for _ in range(N - 1)]\n",
    "    shares += [(secret - sum(shares)) % Q]\n",
    "    return shares\n",
    "\n",
    "\n",
    "def additive_reconstruct(shares):\n",
    "    return sum(shares) % Q\n",
    "\n",
    "\n",
    "def share_tensor(tensor, n):\n",
    "    random_values = [random.randrange(Q) for _ in range(np.prod((tensor.shape + (n - 1,))))]\n",
    "    random_shares = np.array(random_values).reshape(n - 1, tensor.shape[0], tensor.shape[1])\n",
    "    n_shares = np.concatenate([random_shares, [(tensor - random_shares.sum(axis=0)) % Q]])\n",
    "    return n_shares\n",
    "\n",
    "\n",
    "def additive_reconstruct_tensor(shares):\n",
    "    secrets = shares.sum(axis=0) % Q\n",
    "    return secrets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fixed Point Conversion Testing\n",
    "### Single Values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Float Point: 1.2345\n",
      "Test Fixed Point (Converted): 123450000\n",
      "Test Fixed Point Shares: [283714451870265387167790872036476803, 1549885977164866690853642226773371265, 823403060499413029893799710144191975]\n",
      "Test Fixed Point Additive Reconstructed: 123450000\n",
      "Test Float Point Reconstructed: 1.2345\n"
     ]
    }
   ],
   "source": [
    "test_float_point = 1.2345\n",
    "\n",
    "test_fixed_point = fixed_point(test_float_point)\n",
    "\n",
    "test_fixed_point_shares = additive_share(test_fixed_point, 3)\n",
    "\n",
    "test_fixed_point_recon = additive_reconstruct(test_fixed_point_shares)\n",
    "\n",
    "test_reconstructed_float_point = float_point(test_fixed_point)\n",
    "\n",
    "print(f\"Test Float Point: {test_float_point}\")\n",
    "print(f\"Test Fixed Point (Converted): {test_fixed_point}\")\n",
    "print(f\"Test Fixed Point Shares: {test_fixed_point_shares}\")\n",
    "print(f\"Test Fixed Point Additive Reconstructed: {test_fixed_point_recon}\")\n",
    "print(f\"Test Float Point Reconstructed: {test_reconstructed_float_point}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tensor Values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float Point Secret\n",
      "[[0.65505991 0.29824622 0.95773975]\n",
      " [0.79589814 0.20183362 0.12633212]\n",
      " [0.74974386 0.14065554 0.20362259]\n",
      " [0.13407458 0.46753325 0.51083551]]\n",
      "\n",
      "Fixed Point Secret (Converted)\n",
      "[[65505991 29824622 95773975]\n",
      " [79589813 20183361 12633212]\n",
      " [74974385 14065553 20362259]\n",
      " [13407458 46753324 51083550]]\n",
      "\n",
      "Float Point Secret Reconstructed\n",
      "[[0.65505991 0.29824622 0.95773975]\n",
      " [0.79589813 0.20183361 0.12633212]\n",
      " [0.74974385 0.14065553 0.20362259]\n",
      " [0.13407458 0.46753324 0.5108355 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fixedPoint = np.vectorize(fixed_point)\n",
    "floatPoint = np.vectorize(float_point)\n",
    "\n",
    "float_point_secret = np.random.rand(4, 3)\n",
    "\n",
    "fixed_point_secret = fixedPoint(float_point_secret)\n",
    "float_point_secret_reconstructed = floatPoint(fixed_point_secret)\n",
    "\n",
    "print(f\"Float Point Secret\\n{float_point_secret}\\n\")\n",
    "print(f\"Fixed Point Secret (Converted)\\n{fixed_point_secret}\\n\")\n",
    "print(f\"Float Point Secret Reconstructed\\n{float_point_secret_reconstructed}\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Integer Tensor Additive Sharing Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secret\n",
      "[[ 9 18 19]\n",
      " [12  3  1]\n",
      " [16 11  8]\n",
      " [ 5 19 11]]\n",
      "\n",
      "Reconstructed Secret\n",
      "[[9 18 19]\n",
      " [12 3 1]\n",
      " [16 11 8]\n",
      " [5 19 11]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "secret_int = np.random.randint(1, 20, (4, 3))\n",
    "print(f\"Secret\\n{secret_int}\\n\")\n",
    "\n",
    "test_shares = share_tensor(secret_int, N)\n",
    "# print(f\"Secret Shares: \\n{test_shares}\\n\")\n",
    "\n",
    "secret_reconstructed = additive_reconstruct_tensor(test_shares)\n",
    "\n",
    "print(f\"Reconstructed Secret\\n{secret_reconstructed}\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Float Tensor Additive Sharing Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secret\n",
      "[[0.77737174 0.33138294 0.60599308]\n",
      " [0.35832953 0.2501239  0.81509734]\n",
      " [0.20232609 0.26429611 0.30046175]\n",
      " [0.38385028 0.81012827 0.0835832 ]]\n",
      "\n",
      "Fixed Point Secret\n",
      "[[65505991 29824622 95773975]\n",
      " [79589813 20183361 12633212]\n",
      " [74974385 14065553 20362259]\n",
      " [13407458 46753324 51083550]]\n",
      "\n",
      "Reconstructed Fixed Point Secret\n",
      "[[65505991 29824622 95773975]\n",
      " [79589813 20183361 12633212]\n",
      " [74974385 14065553 20362259]\n",
      " [13407458 46753324 51083550]]\n",
      "\n",
      "Reconstructed Float Point Secret\n",
      "[[0.65505991 0.29824622 0.95773975]\n",
      " [0.79589813 0.20183361 0.12633212]\n",
      " [0.74974385 0.14065553 0.20362259]\n",
      " [0.13407458 0.46753324 0.5108355 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "secret_int = np.random.rand(4, 3)\n",
    "print(f\"Secret\\n{secret_int}\\n\")\n",
    "\n",
    "fixed_point_secret = fixedPoint(float_point_secret)\n",
    "print(f\"Fixed Point Secret\\n{fixed_point_secret}\\n\")\n",
    "\n",
    "fixed_point_test_shares = share_tensor(fixed_point_secret, N)\n",
    "fixed_point_secret_reconstructed = additive_reconstruct_tensor(fixed_point_test_shares)\n",
    "\n",
    "print(f\"Reconstructed Fixed Point Secret\\n{fixed_point_secret_reconstructed}\\n\")\n",
    "print(f\"Reconstructed Float Point Secret\\n{floatPoint(fixed_point_secret_reconstructed)}\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SharedScalar\n",
      " - Share(-8901149815987637370, 'kofi', Q=None)\n",
      " - Share(7242444370969791757, 'ama', Q=None)\n",
      " - Share(1658705445017885613, 'kojo', Q=None)\n",
      "SharedScalar\n",
      " - Share(-8079527532602339583, 'kofi', Q=None)\n",
      " - Share(-3732971907603757191, 'ama', Q=None)\n",
      " - Share(-6634244633503439842, 'kojo', Q=None)\n",
      "SharedScalar\n",
      " - Share(-9111602508798889657, 'kofi', Q=None)\n",
      " - Share(4878177505536876685, 'ama', Q=None)\n",
      " - Share(4233425003261992972, 'kojo', Q=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": "PrivateScalar(35000, 'aggregator')"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = [\n",
    "    [\"kofi\", \"ama\", \"kojo\"],\n",
    "    [40000, 15000, -20000],\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "Secret sharing process\n",
    "\"\"\"\n",
    "#\n",
    "aggregator = VirtualMachine(\"aggregator\")\n",
    "\n",
    "# Edge Devices\n",
    "nodes = [VirtualMachine(node_id) for node_id in parameters[0]]\n",
    "\n",
    "# Tensors of edge devices\n",
    "node_values = [PrivateScalar(tensor, node) for tensor, node in zip(parameters[1], nodes)]\n",
    "\n",
    "# Nodes with their local shares\n",
    "exchanged_shares = []\n",
    "\n",
    "for value in node_values:\n",
    "    exchanged_shares.append(value.share(nodes))\n",
    "\n",
    "for node in exchanged_shares:\n",
    "    print(node)\n",
    "\n",
    "shared_sum = sum(exchanged_shares)\n",
    "shared_sum.reconstruct(aggregator)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sum\n",
      "[[152583182 109968855 137267852]\n",
      " [182664572  74534249 117207282]\n",
      " [127671913 203622001 132459203]\n",
      " [166374039 167008605  91701328]]\n",
      "\n",
      "Shared Sum\n",
      "PrivateScalar([[152583182 109968855 137267852]\n",
      " [182664572 74534249 117207282]\n",
      " [127671913 203622001 132459203]\n",
      " [166374039 167008605 91701328]], 'fl_server')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tensor_1 = fixedPoint(np.random.rand(4, 3))\n",
    "tensor_2 = fixedPoint(np.random.rand(4, 3))\n",
    "tensor_3 = fixedPoint(np.random.rand(4, 3))\n",
    "\n",
    "fl_node_weights = [\n",
    "    [\"kofi\", \"ama\", \"kojo\"],\n",
    "    [tensor_1, tensor_2, tensor_3],\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "Secret sharing process\n",
    "\"\"\"\n",
    "#\n",
    "fl_server = VirtualMachine(\"fl_server\")\n",
    "\n",
    "# Edge Devices\n",
    "fl_nodes = [VirtualMachine(node_id) for node_id in fl_node_weights[0]]\n",
    "\n",
    "# Tensors of edge devices\n",
    "fl_node_values = [PrivateScalar(tensor, node) for tensor, node in zip(fl_node_weights[1], fl_nodes)]\n",
    "\n",
    "# Nodes with their local shares\n",
    "fl_exchanged_shares = []\n",
    "\n",
    "for value in fl_node_values:\n",
    "    fl_exchanged_shares.append(value.share_tensor(fl_nodes, Q))\n",
    "\n",
    "shared_sum = sum(fl_exchanged_shares)\n",
    "shared_sum.reconstruct(fl_server)\n",
    "\n",
    "original_sum = np.array(fl_node_weights[1]).sum(axis=0)\n",
    "\n",
    "print(f\"Original Sum\\n{original_sum}\\n\")\n",
    "print(f\"Shared Sum\\n{fl_server.objects[-1]}\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pytorch Integration\n",
    "### Simple ANN in pytorch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "NUM_CLIENTS = 5\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "\n",
    "def load_datasets():\n",
    "    dataset = pd.read_csv(\"./iris.csv\")\n",
    "    dataset.columns = [\"sepal length (cm)\",\n",
    "                       \"sepal width (cm)\",\n",
    "                       \"petal length (cm)\",\n",
    "                       \"petal width (cm)\",\n",
    "                       \"species\"]\n",
    "\n",
    "    mappings = {\n",
    "        \"Iris-setosa\": 0,\n",
    "        \"Iris-versicolor\": 1,\n",
    "        \"Iris-virginica\": 2\n",
    "    }\n",
    "    dataset[\"species\"] = dataset[\"species\"].apply(lambda x: mappings[x])\n",
    "\n",
    "    X = dataset.drop(\"species\", axis=1).values\n",
    "    y = dataset[\"species\"].values\n",
    "\n",
    "    inputs = torch.tensor(X, dtype=torch.float32)\n",
    "    targets = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    ds = TensorDataset(inputs, targets)\n",
    "\n",
    "    train_size = 100\n",
    "    test_size = len(ds) - train_size\n",
    "\n",
    "    train_ds, test_ds = random_split(ds, [train_size, test_size], torch.Generator().manual_seed(42))\n",
    "\n",
    "    # Split training set into 10 partitions to simulate the individual dataset\n",
    "    partition_size = len(train_ds) // NUM_CLIENTS\n",
    "    lengths = [partition_size] * NUM_CLIENTS\n",
    "    datasets = random_split(train_ds, lengths, torch.Generator().manual_seed(42))\n",
    "\n",
    "    # Split each partition into train/val and create DataLoader\n",
    "    trainloaders = []\n",
    "    valloaders = []\n",
    "    for ds in datasets:\n",
    "        len_val = len(ds) // 10  # 10 % validation set\n",
    "        len_train = len(ds) - len_val\n",
    "        lengths = [len_train, len_val]\n",
    "        ds_train, ds_val = random_split(ds, lengths, torch.Generator().manual_seed(42))\n",
    "        trainloaders.append(DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True))\n",
    "        valloaders.append(DataLoader(ds_val, batch_size=BATCH_SIZE))\n",
    "\n",
    "    testloader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "\n",
    "trainloaders, valloaders, testloader = load_datasets()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "input_dim = 4\n",
    "hidden_dim = 25\n",
    "output_dim = 3\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_features, hidden_layer, output_features):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_features, hidden_layer)\n",
    "        self.out = nn.Linear(hidden_layer, output_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def train(net, trainloader, epochs: int, verbose=False):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "    net.train()\n",
    "    training_start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        correct, total, epoch_loss = 0, 0, 0.0\n",
    "        for X_train, y_train in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = net.forward(X_train)\n",
    "            loss = criterion(y_pred, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Metrics\n",
    "            epoch_loss += loss\n",
    "            total += y_train.size(0)\n",
    "            correct += (torch.max(y_pred.data, 1)[1] == y_train).sum().item()\n",
    "\n",
    "        epoch_loss /= len(trainloader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}: train loss:  {epoch_loss}, \"\n",
    "                f\"accuracy: {epoch_acc}, \"\n",
    "                f\"time taken: {time.time() - training_start_time}\"\n",
    "            )\n",
    "\n",
    "\n",
    "def test(net, testloader):\n",
    "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_train, y_train in testloader:\n",
    "            outputs = net(X_train)\n",
    "            loss += criterion(outputs, y_train).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += y_train.size(0)\n",
    "            correct += (predicted == y_train).sum().item()\n",
    "    loss /= len(testloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return loss, accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Float",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[22], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m valloader \u001B[38;5;241m=\u001B[39m valloaders[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m      3\u001B[0m net \u001B[38;5;241m=\u001B[39m Model(input_dim, hidden_dim, output_dim)\u001B[38;5;241m.\u001B[39mto(DEVICE)\n\u001B[1;32m----> 5\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrainloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m loss, accuracy \u001B[38;5;241m=\u001B[39m test(net, testloader)\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFinal test set performance:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124mloss \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124maccuracy \u001B[39m\u001B[38;5;132;01m{\u001B[39;00maccuracy\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[21], line 15\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(net, trainloader, epochs, verbose)\u001B[0m\n\u001B[0;32m     13\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     14\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m net\u001B[38;5;241m.\u001B[39mforward(X_train)\n\u001B[1;32m---> 15\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mcriterion\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     17\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[1;32m~\\.conda\\envs\\machine_learning_0\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\.conda\\envs\\machine_learning_0\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1174\u001B[0m, in \u001B[0;36mCrossEntropyLoss.forward\u001B[1;34m(self, input, target)\u001B[0m\n\u001B[0;32m   1173\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m-> 1174\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1175\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1176\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\machine_learning_0\\lib\\site-packages\\torch\\nn\\functional.py:3026\u001B[0m, in \u001B[0;36mcross_entropy\u001B[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001B[0m\n\u001B[0;32m   3024\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3025\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[1;32m-> 3026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_Reduction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_enum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: expected scalar type Long but found Float"
     ]
    }
   ],
   "source": [
    "trainloader = trainloaders[0]\n",
    "valloader = valloaders[0]\n",
    "net = Model(input_dim, hidden_dim, output_dim).to(DEVICE)\n",
    "\n",
    "train(net, trainloader, 5, True)\n",
    "\n",
    "loss, accuracy = test(net, testloader)\n",
    "print(f\"Final test set performance:\\n\\tloss {loss}\\n\\taccuracy {accuracy}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "model = Model(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss: 1.50877154\n",
      "epoch:  1  loss: 1.31337249\n",
      "epoch:  2  loss: 1.19190168\n",
      "epoch:  3  loss: 1.12893331\n",
      "epoch:  4  loss: 1.11746204\n",
      "epoch:  5  loss: 1.12976313\n",
      "epoch:  6  loss: 1.13385344\n",
      "epoch:  7  loss: 1.12027764\n",
      "epoch:  8  loss: 1.09459150\n",
      "epoch:  9  loss: 1.06500697\n",
      "epoch: 10  loss: 1.03642476\n",
      "epoch: 11  loss: 1.00935745\n",
      "epoch: 12  loss: 0.98309314\n",
      "epoch: 13  loss: 0.95777440\n",
      "epoch: 14  loss: 0.93439484\n",
      "epoch: 15  loss: 0.91399240\n",
      "epoch: 16  loss: 0.89615339\n",
      "epoch: 17  loss: 0.87954271\n",
      "epoch: 18  loss: 0.86441445\n",
      "epoch: 19  loss: 0.84770775\n",
      "epoch: 20  loss: 0.82643974\n",
      "epoch: 21  loss: 0.80183363\n",
      "epoch: 22  loss: 0.77610856\n",
      "epoch: 23  loss: 0.75138932\n",
      "epoch: 24  loss: 0.72907358\n",
      "epoch: 25  loss: 0.70935827\n",
      "epoch: 26  loss: 0.69152653\n",
      "epoch: 27  loss: 0.67423224\n",
      "epoch: 28  loss: 0.65653789\n",
      "epoch: 29  loss: 0.63821149\n",
      "epoch: 30  loss: 0.61967319\n",
      "epoch: 31  loss: 0.60161477\n",
      "epoch: 32  loss: 0.58465350\n",
      "epoch: 33  loss: 0.56916916\n",
      "epoch: 34  loss: 0.55497640\n",
      "epoch: 35  loss: 0.54182780\n",
      "epoch: 36  loss: 0.52953112\n",
      "epoch: 37  loss: 0.51785702\n",
      "epoch: 38  loss: 0.50667769\n",
      "epoch: 39  loss: 0.49589542\n",
      "epoch: 40  loss: 0.48547664\n",
      "epoch: 41  loss: 0.47546101\n",
      "epoch: 42  loss: 0.46593276\n",
      "epoch: 43  loss: 0.45701739\n",
      "epoch: 44  loss: 0.44875935\n",
      "epoch: 45  loss: 0.44109419\n",
      "epoch: 46  loss: 0.43381870\n",
      "epoch: 47  loss: 0.42675596\n",
      "epoch: 48  loss: 0.41976362\n",
      "epoch: 49  loss: 0.41284534\n",
      "epoch: 50  loss: 0.40606421\n",
      "epoch: 51  loss: 0.39945102\n",
      "epoch: 52  loss: 0.39298120\n",
      "epoch: 53  loss: 0.38660812\n",
      "epoch: 54  loss: 0.38031521\n",
      "epoch: 55  loss: 0.37412387\n",
      "epoch: 56  loss: 0.36807063\n",
      "epoch: 57  loss: 0.36214364\n",
      "epoch: 58  loss: 0.35629091\n",
      "epoch: 59  loss: 0.35045293\n",
      "epoch: 60  loss: 0.34462169\n",
      "epoch: 61  loss: 0.33881670\n",
      "epoch: 62  loss: 0.33306205\n",
      "epoch: 63  loss: 0.32735151\n",
      "epoch: 64  loss: 0.32166442\n",
      "epoch: 65  loss: 0.31599823\n",
      "epoch: 66  loss: 0.31037581\n",
      "epoch: 67  loss: 0.30482066\n",
      "epoch: 68  loss: 0.29933402\n",
      "epoch: 69  loss: 0.29389122\n",
      "epoch: 70  loss: 0.28848591\n",
      "epoch: 71  loss: 0.28313175\n",
      "epoch: 72  loss: 0.27784166\n",
      "epoch: 73  loss: 0.27260944\n",
      "epoch: 74  loss: 0.26742184\n",
      "epoch: 75  loss: 0.26228210\n",
      "epoch: 76  loss: 0.25720638\n",
      "epoch: 77  loss: 0.25220346\n",
      "epoch: 78  loss: 0.24726783\n",
      "epoch: 79  loss: 0.24239935\n",
      "epoch: 80  loss: 0.23759408\n",
      "epoch: 81  loss: 0.23285969\n",
      "epoch: 82  loss: 0.22819218\n",
      "epoch: 83  loss: 0.22358146\n",
      "epoch: 84  loss: 0.21902293\n",
      "epoch: 85  loss: 0.21452846\n",
      "epoch: 86  loss: 0.21009664\n",
      "epoch: 87  loss: 0.20572011\n",
      "epoch: 88  loss: 0.20139749\n",
      "epoch: 89  loss: 0.19713096\n",
      "epoch: 90  loss: 0.19292486\n",
      "epoch: 91  loss: 0.18880835\n",
      "epoch: 92  loss: 0.18477014\n",
      "epoch: 93  loss: 0.18092334\n",
      "epoch: 94  loss: 0.17722650\n",
      "epoch: 95  loss: 0.17361183\n",
      "epoch: 96  loss: 0.17003179\n",
      "epoch: 97  loss: 0.16649750\n",
      "epoch: 98  loss: 0.16303445\n",
      "epoch: 99  loss: 0.15966417\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    y_pred = model.forward(X_train)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    losses.append(loss)\n",
    "    print(f'epoch: {i:2}  loss: {loss.item():10.8f}')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
