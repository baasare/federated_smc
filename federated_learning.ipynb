{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Centralized Learning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu using PyTorch 1.13.1+cpu and Flower 1.1.0\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import List, Tuple\n",
    "import time\n",
    "import flwr as fl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from flwr.server.strategy import FedAvg\n",
    "from torchvision.models._api import Weights\n",
    "from torchvision.transforms import transforms\n",
    "from flwr.common import Metrics, parameters_to_ndarrays, ndarrays_to_parameters, FitRes\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "from tinysmpc import VirtualMachine, PrivateScalar\n",
    "from tinysmpc.fixed_point import fixed_point, float_point\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")  # Try \"cuda\" to train on GPU\n",
    "print(f\"Training on {DEVICE} using PyTorch {torch.__version__} and Flower {fl.__version__}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "CLASSES = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "NUM_CLIENTS = 5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition Size 10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "def load_datasets():\n",
    "    # Download and transform CIFAR-10 (train and test)\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    )\n",
    "    trainset = CIFAR10(\"./dataset\", train=True, download=False, transform=transform)\n",
    "    testset = CIFAR10(\"./dataset\", train=False, download=False, transform=transform)\n",
    "\n",
    "    # Split training set into 10 partitions to simulate the individual dataset\n",
    "    partition_size = len(trainset) // NUM_CLIENTS\n",
    "    print(f\"Partition Size {partition_size}\")\n",
    "    lengths = [partition_size] * NUM_CLIENTS\n",
    "    datasets = random_split(trainset, lengths, torch.Generator().manual_seed(42))\n",
    "\n",
    "    # Split each partition into train/val and create DataLoader\n",
    "    trainloaders = []\n",
    "    valloaders = []\n",
    "    for ds in datasets:\n",
    "        len_val = len(ds) // 10  # 10 % validation set\n",
    "        len_train = len(ds) - len_val\n",
    "        lengths = [len_train, len_val]\n",
    "        ds_train, ds_val = random_split(ds, lengths, torch.Generator().manual_seed(42))\n",
    "        trainloaders.append(DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True))\n",
    "        valloaders.append(DataLoader(ds_val, batch_size=BATCH_SIZE))\n",
    "    testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "\n",
    "trainloaders, valloaders, testloader = load_datasets()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "images, labels = next(iter(trainloaders[0]))\n",
    "\n",
    "# Reshape and convert images to a NumPy array\n",
    "# matplotlib requires images with the shape (height, width, 3)\n",
    "images = images.permute(0, 2, 3, 1).numpy()\n",
    "# Denormalize\n",
    "images = images / 2 + 0.5\n",
    "\n",
    "# Create a figure and a grid of subplots\n",
    "fig, axs = plt.subplots(4, 8, figsize=(12, 6))\n",
    "\n",
    "# Loop over the images and plot them\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    ax.imshow(images[i])\n",
    "    ax.set_title(CLASSES[labels[i]])\n",
    "    ax.axis('off')\n",
    "\n",
    "# Show the plot\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def train(net, trainloader, epochs: int, verbose=False):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    net.train()\n",
    "    training_start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        correct, total, epoch_loss = 0, 0, 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "            loss = criterion(net(images), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Metrics\n",
    "            epoch_loss += loss\n",
    "            total += labels.size(0)\n",
    "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "        epoch_loss /= len(trainloader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}: train loss:  {epoch_loss}, accuracy: {epoch_acc}, time taken: {time.time() - training_start_time}\")\n",
    "\n",
    "\n",
    "def test(net, testloader):\n",
    "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    loss /= len(testloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return loss, accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss:  0.06070292741060257, accuracy: 0.289, time taken: 6.787944793701172\n",
      "Epoch 2: train loss:  0.05182642862200737, accuracy: 0.3993333333333333, time taken: 15.387065649032593\n",
      "Epoch 3: train loss:  0.0475374199450016, accuracy: 0.451, time taken: 22.307281970977783\n",
      "Epoch 4: train loss:  0.0448681116104126, accuracy: 0.47988888888888886, time taken: 28.740000009536743\n",
      "Epoch 5: train loss:  0.04237360879778862, accuracy: 0.5161111111111111, time taken: 34.9176459312439\n",
      "Final test set performance:\n",
      "\tloss 0.04520877415537834\n",
      "\taccuracy 0.477\n"
     ]
    }
   ],
   "source": [
    "trainloader = trainloaders[0]\n",
    "valloader = valloaders[0]\n",
    "net = Net().to(DEVICE)\n",
    "\n",
    "train(net, trainloader, 5, True)\n",
    "\n",
    "loss, accuracy = test(net, testloader)\n",
    "print(f\"Final test set performance:\\n\\tloss {loss}\\n\\taccuracy {accuracy}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(net.parameters)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net.state_dict()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parameters = [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "len(parameters)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net.state_dict()['fc3.weight']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "numpy_weights = net.state_dict()['fc3.weight'].cpu().detach().numpy()\n",
    "print(numpy_weights.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(numpy_weights[:3].shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from flwr.common import ndarray_to_bytes\n",
    "\n",
    "tensors = [ndarray_to_bytes(ndarray) for ndarray in net.state_dict()['fc3.weight']]\n",
    "tensors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Federated Learning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def get_parameters(net) -> List[np.ndarray]:\n",
    "    parameters = [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def set_parameters(net, parameters: List[np.ndarray]):\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Local Client"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, net, trainloader, valloader):\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        return get_parameters(self.net)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        set_parameters(self.net, parameters)\n",
    "        train(self.net, self.trainloader, epochs=1)\n",
    "        return get_parameters(self.net), len(self.trainloader), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        set_parameters(self.net, parameters)\n",
    "        loss, accuracy = test(self.net, self.valloader)\n",
    "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def client_fn(cid: str) -> FlowerClient:\n",
    "    \"\"\"Create a Flower client representing a single organization.\"\"\"\n",
    "\n",
    "    # Load model\n",
    "    net = Net().to(DEVICE)\n",
    "\n",
    "    # Load data (CIFAR-10)\n",
    "    # Note: each client gets a different trainloader/valloader, so each client\n",
    "    # will train and evaluate on their own unique data\n",
    "    trainloader = trainloaders[int(cid)]\n",
    "    valloader = valloaders[int(cid)]\n",
    "\n",
    "    # Create a  single Flower client representing a single organization\n",
    "    return FlowerClient(net, trainloader, valloader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from typing import Optional, Dict\n",
    "\n",
    "\n",
    "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
    "    # Multiply accuracy of each client by number of examples used\n",
    "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
    "    examples = [num_examples for num_examples, _ in metrics]\n",
    "\n",
    "    # Aggregate and return custom metric (weighted average)\n",
    "    return {\"accuracy\": sum(accuracies) / sum(examples)}\n",
    "\n",
    "\n",
    "# The `evaluate` function will be by Flower called after every round\n",
    "def evaluate(server_round: int, parameters: fl.common.NDArrays, config: Dict[str, fl.common.Scalar]) -> Optional[\n",
    "    Tuple[float, Dict[str, fl.common.Scalar]]]:\n",
    "    net = Net()\n",
    "    valloader = valloaders[0]\n",
    "    set_parameters(net, parameters)  # Update model with the latest parameters\n",
    "    loss, accuracy = test(net, valloader)\n",
    "    print(f\"Server-side evaluation loss {loss} / accuracy {accuracy}\")\n",
    "    return loss, {\"accuracy\": accuracy}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "Q = 2657003489534545107915232808830590043\n",
    "fixedPoint = np.vectorize(fixed_point)\n",
    "floatPoint = np.vectorize(float_point)\n",
    "\n",
    "\n",
    "class FedAvgSmc(FedAvg):\n",
    "    def aggregate_fit(\n",
    "            self,\n",
    "            rnd: int,\n",
    "            results: List[Tuple[fl.server.client_proxy.ClientProxy, FitRes]],\n",
    "            failures: List[BaseException],\n",
    "    ) -> Optional[Weights]:\n",
    "        if not results:\n",
    "            return None\n",
    "\n",
    "        clients = []\n",
    "        fit_results = []\n",
    "\n",
    "        for client, fit_result in results:\n",
    "            clients.append(client)\n",
    "            fit_results.append(fit_result)\n",
    "\n",
    "        fit_res_ndarray_parameters = [parameters_to_ndarrays(result.parameters) for result in\n",
    "                                      fit_results]  # list of clients, each with a list of layers(each layer representing weights)\n",
    "\n",
    "        fl_nodes = [VirtualMachine(f\"Client: {client.cid}\") for client in clients]\n",
    "\n",
    "        num_layers = len(fit_results[0].parameters.tensors)\n",
    "\n",
    "        layers_weights = {}\n",
    "\n",
    "        layers_weights_smc = {}\n",
    "\n",
    "        for layer in range(num_layers):  #loop through number of layers\n",
    "            layers_weights[f\"layer_{layer}\"] = []\n",
    "            for weights in fit_res_ndarray_parameters:\n",
    "                layers_weights[f\"layer_{layer}\"].append(fixedPoint(weights[layer]))\n",
    "\n",
    "        for layer in range(num_layers):\n",
    "            fl_node_values = [PrivateScalar(tensor, node) for tensor, node in\n",
    "                              zip(layers_weights[f'layer_{layer}'], fl_nodes)]\n",
    "\n",
    "            fl_exchanged_shares = []\n",
    "            fl_exchanged_shares_list = []\n",
    "            layers_weights_smc[f\"layer_{layer}\"] = []\n",
    "\n",
    "            for value in fl_node_values:\n",
    "                fl_exchanged_shares.append(value.share_tensor(fl_nodes, Q))\n",
    "\n",
    "            for client_shares in fl_exchanged_shares:\n",
    "                fl_exchanged_shares_list = [share.value for share in client_shares.shares]\n",
    "\n",
    "            layers_weights_smc[f'layer_{layer}'] = floatPoint(fl_exchanged_shares_list)\n",
    "\n",
    "        for i, client in enumerate(clients):\n",
    "            client_weights = []\n",
    "            for layer in layers_weights_smc.values():\n",
    "                client_weights.append(np.array(layer[i]))\n",
    "            fit_results[i].parameters = ndarrays_to_parameters(client_weights)\n",
    "\n",
    "        results = tuple(zip(clients, fit_results))\n",
    "\n",
    "        return super().aggregate_fit(rnd, results, failures)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flower 2023-01-25 16:40:19,568 | app.py:140 | Starting Flower simulation, config: ServerConfig(num_rounds=2, round_timeout=None)\n",
      "2023-01-25 16:40:25,424\tINFO worker.py:1518 -- Started a local Ray instance.\n",
      "INFO flower 2023-01-25 16:40:29,127 | app.py:174 | Flower VCE: Ray initialized with resources: {'object_store_memory': 1709134233.0, 'memory': 3418268468.0, 'node:127.0.0.1': 1.0, 'CPU': 4.0}\n",
      "INFO flower 2023-01-25 16:40:29,128 | server.py:86 | Initializing global parameters\n",
      "INFO flower 2023-01-25 16:40:29,129 | server.py:270 | Requesting initial parameters from one random client\n",
      "INFO flower 2023-01-25 16:40:33,573 | server.py:274 | Received initial parameters from one random client\n",
      "INFO flower 2023-01-25 16:40:33,574 | server.py:88 | Evaluating initial parameters\n",
      "INFO flower 2023-01-25 16:40:34,003 | server.py:91 | initial parameters (loss, other metrics): 0.07369909429550171, {'accuracy': 0.098}\n",
      "INFO flower 2023-01-25 16:40:34,004 | server.py:101 | FL starting\n",
      "DEBUG flower 2023-01-25 16:40:34,005 | server.py:215 | fit_round 1: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server-side evaluation loss 0.07369909429550171 / accuracy 0.098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flower 2023-01-25 16:41:03,629 | server.py:229 | fit_round 1 received 5 results and 0 failures\n",
      "WARNING flower 2023-01-25 16:41:06,030 | fedavg.py:242 | No fit_metrics_aggregation_fn provided\n",
      "INFO flower 2023-01-25 16:41:06,399 | server.py:116 | fit progress: (1, 0.059464130878448485, {'accuracy': 0.349}, 32.39383960000009)\n",
      "DEBUG flower 2023-01-25 16:41:06,400 | server.py:165 | evaluate_round 1: strategy sampled 3 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server-side evaluation loss 0.059464130878448485 / accuracy 0.349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flower 2023-01-25 16:41:10,526 | server.py:179 | evaluate_round 1 received 3 results and 0 failures\n",
      "DEBUG flower 2023-01-25 16:41:10,527 | server.py:215 | fit_round 2: strategy sampled 5 clients (out of 5)\n",
      "DEBUG flower 2023-01-25 16:41:37,884 | server.py:229 | fit_round 2 received 5 results and 0 failures\n",
      "INFO flower 2023-01-25 16:41:40,884 | server.py:116 | fit progress: (2, 0.050110796689987185, {'accuracy': 0.453}, 66.87948580000011)\n",
      "DEBUG flower 2023-01-25 16:41:40,885 | server.py:165 | evaluate_round 2: strategy sampled 3 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server-side evaluation loss 0.050110796689987185 / accuracy 0.453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flower 2023-01-25 16:41:47,166 | server.py:179 | evaluate_round 2 received 3 results and 0 failures\n",
      "INFO flower 2023-01-25 16:41:47,167 | server.py:144 | FL finished in 73.16257710000014\n",
      "INFO flower 2023-01-25 16:41:47,168 | app.py:192 | app_fit: losses_distributed [(1, 0.059504249890645344), (2, 0.05002546882629394)]\n",
      "INFO flower 2023-01-25 16:41:47,169 | app.py:193 | app_fit: metrics_distributed {'accuracy': [(1, 0.35433333333333333), (2, 0.42966666666666664)]}\n",
      "INFO flower 2023-01-25 16:41:47,170 | app.py:194 | app_fit: losses_centralized [(0, 0.07369909429550171), (1, 0.059464130878448485), (2, 0.050110796689987185)]\n",
      "INFO flower 2023-01-25 16:41:47,171 | app.py:195 | app_fit: metrics_centralized {'accuracy': [(0, 0.098), (1, 0.349), (2, 0.453)]}\n"
     ]
    },
    {
     "data": {
      "text/plain": "History (loss, distributed):\n\tround 1: 0.059504249890645344\n\tround 2: 0.05002546882629394\nHistory (loss, centralized):\n\tround 0: 0.07369909429550171\n\tround 1: 0.059464130878448485\n\tround 2: 0.050110796689987185\nHistory (metrics, distributed):\n{'accuracy': [(1, 0.35433333333333333), (2, 0.42966666666666664)]}History (metrics, centralized):\n{'accuracy': [(0, 0.098), (1, 0.349), (2, 0.453)]}"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create FedAvg strategy\n",
    "strategy = FedAvgSmc(\n",
    "    fraction_fit=1.0,  # Sample 100% of available clients for training\n",
    "    fraction_evaluate=0.5,  # Sample 50% of available clients for evaluation\n",
    "    min_fit_clients=5,  # Never sample less than 5 clients for training\n",
    "    min_evaluate_clients=3,  # Never sample less than 3 clients for evaluation\n",
    "    min_available_clients=5,  # Wait until 5 clients are available\n",
    "    evaluate_metrics_aggregation_fn=weighted_average,  # pass the metric aggregation function\n",
    "    evaluate_fn=evaluate,  # Pass the evaluation function\n",
    ")\n",
    "\n",
    "# Start simulation\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=NUM_CLIENTS,\n",
    "    config=fl.server.ServerConfig(num_rounds=2),\n",
    "    strategy=strategy,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create FedAvg strategy\n",
    "strategy = FedAvgSmc(\n",
    "    fraction_fit=1.0,  # Sample 100% of available clients for training\n",
    "    fraction_evaluate=0.5,  # Sample 50% of available clients for evaluation\n",
    "    min_fit_clients=5,  # Never sample less than 5 clients for training\n",
    "    min_evaluate_clients=3,  # Never sample less than 3 clients for evaluation\n",
    "    min_available_clients=5,  # Wait until 5 clients are available\n",
    "    evaluate_metrics_aggregation_fn=weighted_average,  # pass the metric aggregation function\n",
    "    evaluate_fn=evaluate,  # Pass the evaluation function\n",
    ")\n",
    "\n",
    "# Start simulation\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=NUM_CLIENTS,\n",
    "    config=fl.server.ServerConfig(num_rounds=10),\n",
    "    strategy=strategy,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
